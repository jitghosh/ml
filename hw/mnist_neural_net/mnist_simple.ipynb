{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "define your neural net model class\n",
    "    - Include a nn.Softmax(dim=1) at the end of the model inside your nn.Sqeuential(). This is missing in the tutorial\n",
    "    - Make sure to make the hidden_layer_Size a parameter to the ctor - so that you can later do parameter search via CV\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "define your train function\n",
    "    - pass the model, the training X data tensor, training y data tensor, loss function, optimizer, num of epochs, learning rate, batch size\n",
    "    - create the TensorDataset\n",
    "    - create the dataloader using the dataset, pass in batch size and shuffle = True\n",
    "    - create blank arrays/lists for storing training loss and training accuracy across all epochs\n",
    "    - for each epoch\n",
    "        - create a blank array/list to store loss for this epoch\n",
    "        - set model to train mode\n",
    "        - for each batch in dalaloader\n",
    "            - call the model with the batch X data and get the predictions\n",
    "            - calculate the loss using the loss function using the predictions and the batch y data (ground truth)\n",
    "            - backprop via loss.backward()\n",
    "            - update weights via optimizer step()\n",
    "            - reset gradients via zero_grad()\n",
    "            - append the loss (use the item() function on the loss - it is a single item tensor ) to the epoch loss \n",
    "        - append the mean epoch loss to the train loss list\n",
    "        - call the model again this time the full training X data and get the predictions for this epoch\n",
    "        - use sklearn accuracy score to calculate training accuracy for the preictions against the full y data (you will need to call detach() and then numpy() on the returned predictions \n",
    "                in the prev step to detach it from gradient calculations and make it numpy, and just numpy() in the y data tensor) \n",
    "\n",
    "    - return the training loss and training accuracy arrays\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "define your test function\n",
    "    - pass the model, test X data, test y data, loss function\n",
    "    - set the model to test mode (eval())\n",
    "    - with no gradient calculations (torch.no_grad())\n",
    "        - call the model with the test X data and get test predictions\n",
    "        - call loss function with the predictions and the test y data and get test loss\n",
    "        - calculate test accuracy the same way as in training\n",
    "    return test loss (.item()) and test accuracy\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load the data from the pickle file\n",
    " - be sure to use a latin1 encoding with pickle.load\n",
    "- create model from the model class you defined earlier\n",
    "- create loss function (CrossEntropyLoss)\n",
    "- create optimizer (SGD) - pass in model parameters (Weights) and learning rate\n",
    "- call train function and store train loss, train accuracy. Make sure to use torch.as_tensor to turn the numpy X, y data into tensors as you pass them\n",
    "- plot train loss and train accuracy across epochs\n",
    "- call test function and get test loss and test accuracy\n",
    "- print test accuracy\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
