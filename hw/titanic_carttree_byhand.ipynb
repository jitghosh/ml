{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements a CART tree for classification using gini impurity to decide on splitting.\n",
    "* The tree only supports numerical input data (float, int etc.)\n",
    "* The tree only supports categorical input data if it is one hot encoded. This way a categorical feature can be used to split based on just two categories (0 or 1) which makes it a lot easier\n",
    "* The ground truth (y values) are either just 0/1 (binary classification) or ordinal (0,1,2,3, etc.) for multi class case\n",
    "* The stopping criterion for splitting:\n",
    "    * If either a specified max depth has been reached or the number of samples at the node is less than or equal to a specified minimum required to split\n",
    "    * If all the labels(predictions) at the node are of the same class\n",
    "    * If the gini cost is not calculated because a split is not possible i.e. every split point that is evaluated results in a situation where one side of the split has no samples\n",
    "* Logic\n",
    "    1. At the root create a node whose gini is calculated utilizing the whole training set. Assign the entire dataset (X and y values) to the root node. Set the root node as the current node\n",
    "    2. Loop through all features for the dataset portion for the current node and try each one as a possible candidate for splitting\n",
    "    3. For each such feature if the feature is categorical (one hot encoded so values are 0 or 1) try splitting between all 0's and 1's and calculate cost\n",
    "    4. If the feature is numeric, then try each value in the column as a possible splitting point and calculate cost\n",
    "    5. From steps 3 and 4 pick the split with minimum cost. Create the left and the right children nodes for the current node based on this split\n",
    "    6. Then recursively repeat from step 2 for the left and right node\n",
    "    7. Stop when stopping criteria is reached\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a class that represents a node. The node class at minimum needs the following properties:\n",
    "1. The portions of the training dataset (X and y) associated with it. For the root node this will be the entire X and y.\n",
    "2. The parent node (None if node is root)\n",
    "3. The node depth\n",
    "4. The right child node\n",
    "5. The left child node\n",
    "6. The dominant class (the class in this node's y data that has the highest count). This will be used to predict the class of a test sample if this node becomes a leaf node and is used for prediction\n",
    "7. The feature index (column index from X) on whihc this node is being split\n",
    "8. Property indicating whether the feature used to split is categorical or not\n",
    "9. The feature value at which the split occurs (only applicable for non-categorical columns. For categorical (one hot encoded) columns the split always occurs such that all 0's go left and the 1's go right)\n",
    "7. The gini score (this is not required - but good to keep around for each node for future visualization etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, parent = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # get the unique classes and their counts from the ground truth (y)\n",
    "        values,counts = np.unique(y,return_counts=True)\n",
    "        # set the class with the highest count as the class to be predicted using this node\n",
    "        self.predict_class = values[np.argmax(counts)]\n",
    "        # calculate gini for this node\n",
    "        self.gini = 1 - np.sum([(y[y == cls].shape[0]/y.shape[0])**2 for cls in values])       \n",
    "        self.parent = parent\n",
    "        # increment depth as needed\n",
    "        self.depth = 1 if parent is None else parent.depth + 1\n",
    "\n",
    "        self.right: Node = None\n",
    "        self.left: Node = None\n",
    "        self.feature_idx = None\n",
    "        self.categorical = None\n",
    "        self.split_at = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is the main tree class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    # constructor\n",
    "    def __init__(self, max_depth = None, min_sample_split = None):\n",
    "        # initialize root node - we do not have one yet\n",
    "        self.head = None\n",
    "        # save the stopping parameters\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        # initialize the class list (we do not have this yet)\n",
    "        self.clsarr = None\n",
    "\n",
    "    '''\n",
    "    This is the main method to fit (train) the tree\n",
    "    Arguments:\n",
    "        X - the features (input columns) of the training dataset\n",
    "        y - the labels (0/1 for binary, ordinals (0,1,2,3...) for multiclass)\n",
    "        categorical_col_idx: the list of column indices that are categorical (one hot encoded)\n",
    "    '''\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, categorical_col_idx: set[int]):\n",
    "        # record the label classes\n",
    "        self.clsarr = np.unique(y)\n",
    "        # create the root node\n",
    "        self.head = Node(X,y)\n",
    "        # start splitting with the root node\n",
    "        self.__split(self.head,categorical_col_idx)\n",
    "        # return the tree again so that we can chain method calls\n",
    "        return self\n",
    "\n",
    "    '''\n",
    "    This method (recursively) splits a node. If the method returns without splitting (stopping conditions met)\n",
    "    then the node is a leaf node\n",
    "    Arguments:\n",
    "        cur_node: Current node under consideration for splitting\n",
    "        categorical_col_idx: the list of column indices that are categorical (one hot encoded)\n",
    "    '''\n",
    "    def __split(self, cur_node: Node, categorical_col_idx: set[int]):        \n",
    "        # stopping conditions\n",
    "        # depth condition check\n",
    "        if (cur_node.parent is not None and self.max_depth is not None and cur_node.parent.depth + 1 > self.max_depth):\n",
    "            return\n",
    "        # min samples required to split check\n",
    "        if(self.min_sample_split is not None and cur_node.X.shape[0] <= self.min_sample_split):\n",
    "            return\n",
    "        # are all the samples at the current node already of a single class ? Then no point splitting.\n",
    "        if (np.unique(cur_node.y).shape[0] == 1):\n",
    "            return\n",
    "\n",
    "        # calculate the split\n",
    "        (feature_idx,split_value) = self.__calculate_feature_to_split_based_on_min_cost(cur_node.X,cur_node.y,categorical_col_idx)\n",
    "        \n",
    "        # for some reason we could not split\n",
    "        if feature_idx == None:\n",
    "            return\n",
    "        \n",
    "        if feature_idx in categorical_col_idx:\n",
    "            # get the row indices from the samples attached to the current node where the categorical (one hot encoded) feature value is 0  \n",
    "            left_indices = np.argwhere(cur_node.X[:,feature_idx] == 0).flatten()\n",
    "            # get the row indices from the samples attached to the current node where the categorical (one hot encoded) feature value is 1\n",
    "            right_indices = np.argwhere(cur_node.X[:,feature_idx] == 1).flatten()\n",
    "        else:\n",
    "            # get the row indices from the samples attached to the current node where the feature value is < split_value \n",
    "            left_indices = np.argwhere(cur_node.X[:,feature_idx] < split_value).flatten()\n",
    "            # get the row indices from the samples attached to the current node where the feature value is >= split_value \n",
    "            right_indices = np.argwhere(cur_node.X[:,feature_idx] >= split_value).flatten()\n",
    "\n",
    "        # if split is going to result in one side being \"no samples\" do not split\n",
    "        if (left_indices.shape[0] == 0 or right_indices.shape[0] == 0):\n",
    "            return\n",
    "        \n",
    "        # update the current node properties - record the index of the feature the node is getting split on, whether that feature is categorical and \n",
    "        # the feature value used in splitting \n",
    "        cur_node.feature_idx = feature_idx\n",
    "        cur_node.categorical = feature_idx in categorical_col_idx\n",
    "        cur_node.split_at = split_value\n",
    "        \n",
    "        # create the left and the right child nodes - associating the appropriate portions of the dataset to each for both the features (X) and labels (y)\n",
    "        cur_node.left = Node(cur_node.X[left_indices,:],cur_node.y[left_indices],cur_node)\n",
    "        cur_node.right = Node(cur_node.X[right_indices,:],cur_node.y[right_indices],cur_node)\n",
    "\n",
    "        # recursively try to split the newly created left and right nodes \n",
    "        self.__split(cur_node.left,categorical_col_idx)\n",
    "        self.__split(cur_node.right,categorical_col_idx)\n",
    "\n",
    "    '''\n",
    "    Given input data (X), ground truth (y - which is either just 0 or 1 for the binary case or ordinal for multi class case) figure \n",
    "    out which input column and which value in that input column can we split at so as to have the minimum cost.\n",
    "    Arguments:\n",
    "        X - input features\n",
    "        y - ground truth label\n",
    "        categorical_col_idx - indices of columns that are categorical\n",
    "    Returns: \n",
    "        tuple of (feature_idx,split_value) where split_value is the x value pointed to by split_point and is None if the feature was categorical\n",
    "    '''\n",
    "    def __calculate_feature_to_split_based_on_min_cost(self, X: np.ndarray, y: np.ndarray, categorical_col_idx: set[int]):\n",
    "        # create an empty array to hold all the costs (and associated values) as we loop through all the features\n",
    "        cost_arr = np.empty((0,3))\n",
    "        # we loop through each feature (column) in X\n",
    "        for feature_idx in range(0,X.shape[1]):\n",
    "            # we take the column at feature_idx and stack that with y making xy (n x 2 in shape) \n",
    "            xy = np.column_stack((X[:,feature_idx],y))\n",
    "\n",
    "            # if the feature is categorical\n",
    "            if feature_idx in categorical_col_idx:\n",
    "                # calculate the cost and gini. Note that the split point we supply is None - since this is a categorical feature\n",
    "                # our assumption is that it has been one hot encoded and hence the only possible values in this feature column is 0 or 1\n",
    "                # and our split will simply be based on that\n",
    "                cost = self.__calculate_cost_and_gini(xy,feature_idx,categorical=True,split_point = None)\n",
    "                # if we got a valid cost and gini - store that\n",
    "                if cost[0,0] != None:\n",
    "                    cost_arr = np.concatenate((cost_arr,cost),axis=0)\n",
    "            else:                \n",
    "                #x_sort_indices = np.argsort(xy,axis=0)[:,0]\n",
    "                #xy = np.take_along_axis(xy,np.column_stack((x_sort_indices,x_sort_indices)),axis=0)\n",
    "                for possible_split_point_idx in range(0,X.shape[0]):\n",
    "                    # get the cost and gini if we were to use this split point\n",
    "                    cost_for_split_point = self.__calculate_cost_and_gini(xy,feature_idx,categorical=False,split_point=possible_split_point_idx)\n",
    "                    # record it\n",
    "                    if cost_for_split_point[0,0] != None:\n",
    "                        cost_arr = np.concatenate((cost_arr,cost_for_split_point))\n",
    "        # we did not get anything back for any of the possible splits\n",
    "        # so we return None's\n",
    "        if cost_arr.shape[0] == 0:\n",
    "            return (None,None)\n",
    "        # of all the cost's we recorded in the above loop for all features\n",
    "        # find the one with the minimum cost. That split is the one we would use\n",
    "        min_cost_idx = np.argmin(cost_arr[:,0],axis = 0)\n",
    "        idx_of_feature_to_split_on = int(cost_arr[min_cost_idx,1])\n",
    "        value_to_split_on = cost_arr[min_cost_idx,2]\n",
    "        # return the index of feature on which we will split \n",
    "        # and the split value (which is the x value at the split_point and is None if the feature was categorical))\n",
    "        return (idx_of_feature_to_split_on,value_to_split_on)\n",
    "    \n",
    "    '''\n",
    "    Calculate the cost and gini impurity for a split\n",
    "\n",
    "    Arguments:\n",
    "        xy - n by 2 dataset where the first column is some feature(x) and the second column is the ground truth label (y)\n",
    "        feature_idx - column index of the feature(x) in the original training data\n",
    "        categorical - is the feature categorical ?\n",
    "        split_point - the row index pointing to the row where we want to split\n",
    "    Returns:\n",
    "        1x5 array of [cost,gini_left,gini_right,feature_idx,split_value (which is the x value pointed to by split_point and is None if the feature was categorical)]\n",
    "    '''\n",
    "    def __calculate_cost_and_gini(self,xy: np.ndarray, feature_idx: int, categorical: bool, split_point: int):\n",
    "        # to calculate gini and cost we only need the y values - so we split them\n",
    "        if categorical:\n",
    "            # left is all values of y where the x == 0, and right is all values of y where x == 1 \n",
    "            # (x is catagorical aka one hot encoded - so possible x values are 0 or 1)\n",
    "            y_left = xy[xy[:,0] == 0,1]\n",
    "            y_right = xy[xy[:,0] == 1,1]\n",
    "        else:\n",
    "            #split_val = (xy[split_point,0] + (xy[split_point+1,0] - xy[split_point,0])/2)\n",
    "            # left is all values of y where  x < (x value at row pointed to by split point) , \n",
    "            # and right is all values of y where  x >= (x value at row pointed to by split point) \n",
    "            y_left = xy[xy[:,0] < xy[split_point,0].item(),1]\n",
    "            y_right = xy[xy[:,0] >= xy[split_point,0].item(),1]\n",
    "\n",
    "        # a split was not possible at that split point because one or the other side resulted in no samples satisfying the split condition\n",
    "        # we return all(3) None values\n",
    "        if (y_left.shape[0] == 0 or y_right.shape[0] == 0):\n",
    "            return np.array([None]*3).reshape(1,-1)\n",
    "        # check the gini and cost formulae\n",
    "        # the probabilities for each class is just the number of y values of that class dividieed by the total number of samples\n",
    "        # to calculate gini we summ the squares of each class probability and subtract from 1\n",
    "        # The cost is then a weighted sum of the two gini values where the weights are the proportion of the number of instances \n",
    "        # in the left/right subset to the total number of samples \n",
    "        gini_left = 1 - np.sum([(y_left[y_left == cls].shape[0]/y_left.shape[0])**2 for cls in self.clsarr])\n",
    "        gini_right = 1 - np.sum([(y_right[y_right == cls].shape[0]/y_right.shape[0])**2 for cls in self.clsarr])\n",
    "        cost = (gini_left * y_left.shape[0]/xy.shape[0]) + (gini_right * y_right.shape[0]/xy.shape[0])\n",
    "        # return as a 1x3 array of [cost,feature_idx,split_value (which is the x value pointed to by split_point and is None if the feature was categorical)]\n",
    "        return np.array([cost, feature_idx, xy[split_point,0].item() if not categorical else None]).reshape(1,-1)\n",
    "    \n",
    "    '''\n",
    "    This is the main method to predict using a trained tree\n",
    "    Arguments:\n",
    "        X - the features (input columns) of the test dataset\n",
    "    Returns:\n",
    "        y - the predicted labels (0/1 for binary, ordinals (0,1,2,3...) for multiclass)\n",
    "    '''\n",
    "    def predict(self,X:np.ndarray) -> np.ndarray:\n",
    "        # apply the internal prediction method once for each row in the test dataset\n",
    "        # apply_along_axis will accumulate all the results into a single y array\n",
    "\n",
    "        return np.apply_along_axis(self.__predict_internal,axis=1,arr=X,use_node = self.head)\n",
    "\n",
    "    '''\n",
    "    This method takes a single x array (1 x n features) of features and returns the \n",
    "    predicted y value (0/1 for binary, ordinals (0,1,2,3...) for multiclass). It recursively\n",
    "    calls itself until a leaf node is reached and a decision can be made\n",
    "    Arguments:\n",
    "        X - one row of test data\n",
    "        use_node - the current node to be used to make a decision\n",
    "    Returns:\n",
    "        A single y value (0/1 for binary, ordinals (0,1,2,3...) for multiclass)\n",
    "    '''\n",
    "    def __predict_internal(self, X:np.ndarray, use_node: Node) -> float:\n",
    "        # base case - we have a reached a leaf node\n",
    "        if (use_node.left is None and use_node.right is None):\n",
    "            # return whatever class this node predicts\n",
    "            return use_node.predict_class\n",
    "        # else choose to go left or right depending on split criterion\n",
    "        elif use_node.categorical:\n",
    "            # if the feature associated with the current node is categorical then go left if the test row has a 0 for that feature else go right\n",
    "            return self.__predict_internal(X,use_node.left if X[use_node.feature_idx].item() == 0 else use_node.right)\n",
    "        else:\n",
    "            # if the feature associated with the current node is non-categorical then go left if the test row has a value for that \n",
    "            # feature that is less than the split value associated with this node else go right\n",
    "            return self.__predict_internal(X,use_node.left if X[use_node.feature_idx].item() < use_node.split_at else use_node.right)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 0.9057851239669421\n",
      "train precision = 0.8081632653061225\n",
      "train recall = 0.9519230769230769\n",
      "test accuracy = 0.7850467289719626\n",
      "test precision = 0.6046511627906976\n",
      "test recall = 0.8125\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"c:/pix/ml/titanic/train.csv\")\n",
    "# drop columns that are meaningless for this\n",
    "df_data = df_data.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"],axis=1).dropna()\n",
    "# get the X data\n",
    "df_X = df_data.drop(\"Survived\",axis=1)\n",
    "# get the y data\n",
    "df_y = df_data[\"Survived\"]\n",
    "# one hot encode categoricals\n",
    "df_X = pd.get_dummies(df_X,columns=[\"Pclass\",\"Sex\",\"Embarked\"])\n",
    "# make all X data numeric\n",
    "df_X = df_X.astype(float)\n",
    "# split\n",
    "train_X,test_X,train_y,test_y = train_test_split(df_X,df_y,test_size=0.15,stratify=df_y,shuffle=True,random_state=1234)\n",
    "\n",
    "# play around with the stopping conditions\n",
    "tree = DecisionTreeClassifier(min_sample_split=12, max_depth=16)\n",
    "# fit the tree\n",
    "tree.fit(train_X.values,train_y.values,categorical_col_idx=range(4,12))\n",
    "# predict with the training set\n",
    "predictions_train = tree.predict(train_X.values)\n",
    "print(f\"train accuracy = {accuracy_score(predictions_train,train_y.values)}\")\n",
    "print(f\"train precision = {precision_score(predictions_train,train_y.values)}\")\n",
    "print(f\"train recall = {recall_score(predictions_train,train_y.values)}\")\n",
    "# predict with the test set\n",
    "predictions_test = tree.predict(test_X.values)\n",
    "print(f\"test accuracy = {accuracy_score(predictions_test,test_y.values)}\")\n",
    "print(f\"test precision = {precision_score(predictions_test,test_y.values)}\")\n",
    "print(f\"test recall = {recall_score(predictions_test,test_y.values)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  7],\n",
       "       [ 9, 12],\n",
       "       [11, 22]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[9,7],[1,22],[11,12]])\n",
    "sort_indices = np.argsort(a,axis=0)[:,0]\n",
    "np.sort(a,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 22],\n",
       "       [ 9,  7],\n",
       "       [11, 12]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_indices = np.column_stack((sort_indices,sort_indices))\n",
    "np.take_along_axis(a,sort_indices,axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
