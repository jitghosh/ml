{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "colnames = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n",
    "df_data = pd.read_csv('processed.cleveland.data',names=colnames, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data['num'] = df_data['num'].map(lambda x: 1 if x >= 1 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data.loc[df_data['cp'] == '?','cp'] = stats.mode(pd.to_numeric(df_data['cp'].loc[df_data['cp'] != '?']))[0]\n",
    "df_data.loc[df_data['thal'] == '?','thal'] = stats.mode(pd.to_numeric(df_data['thal'].loc[df_data['thal'] != '?']))[0]\n",
    "df_data['thal'] = pd.to_numeric(df_data['thal'])\n",
    "df_data.loc[df_data['ca'] == '?','ca'] = stats.mode(pd.to_numeric(df_data['ca'].loc[df_data['ca'] != '?']))[0]\n",
    "df_data['ca'] = pd.to_numeric(df_data['ca'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data = pd.get_dummies(df_data,columns=['sex','cp','fbs','restecg','exang','slope','thal'],dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,test_X,train_y,test_y = train_test_split(df_data.drop(['num'],axis=1),df_data['num'],shuffle=True, test_size=0.2, stratify=df_data['num'],random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = train_X.join(train_y)\n",
    "df_test = test_X.join(test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean vectors of two classes (eqn 4.21 Bishop)\n",
    "\n",
    "$$\\textbf{m}_1 = \\frac{1}{N_1}\\sum_{n \\in C_1}\\textbf{x}_n, \\qquad \\textbf{m}_2 = \\frac{1}{N_2}\\sum_{n \\in C_2}\\textbf{x}_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.expand_dims(np.mean(df_train.loc[df_train['num'] == 0].drop(['num'],axis=1),axis=0),1)\n",
    "m2 = np.expand_dims(np.mean(df_train.loc[df_train['num'] == 1].drop(['num'],axis=1),axis=0),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within class covariance matrix (eqn 4.28 Bishop)\n",
    "$$\n",
    "\\textbf{S}_W = \\sum_{n \\in C_1}(\\textbf{x}_n - \\textbf{m}_1)(\\textbf{x}_n - \\textbf{m}_1)^T + \\sum_{n \\in C_2}(\\textbf{x}_n - \\textbf{m}_2)(\\textbf{x}_n - \\textbf{m}_2)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X_C1 = df_train.loc[df_train['num'] == 0,:].drop(['num'],axis=1)\n",
    "df_train_X_C2 = df_train.loc[df_train['num'] == 1,:].drop(['num'],axis=1)\n",
    "\n",
    "sw = np.dot((df_train_X_C1 - m1.T).T,(df_train_X_C1 - m1.T)) + np.dot((df_train_X_C2 - m2.T).T,(df_train_X_C2 - m2.T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w is obtained using (Eqn 4.30 Bishop)\n",
    "$$\n",
    "\\textbf{w} \\propto \\textbf{S}_W^{-1}(\\textbf{m}_2 - \\textbf{m}_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.dot(np.linalg.inv(sw),(m2 - m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing the fact that the Fisher criterion is a special case of least squares approach to linear discrimination (section 4.1.5 Bishop) we can say that a vector x should be classified as class C1 if $y(x) = w^T(x - m)$ is > 0 and C2 otherwise, where m is the mean vector for the entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.expand_dims(np.mean(df_train.drop(['num'],axis=1),axis=0),1)\n",
    "test_y_hats = np.dot((df_test.drop(['num'],axis=1) - m.T),w)\n",
    "test_y_predictions = (test_y_hats > 0).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6885245901639344\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_y_predictions,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig,ax  = plt.subplots(nrows=1,ncols = 1)\n",
    "ax.scatter()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
