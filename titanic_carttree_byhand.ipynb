{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements a CART tree for classification using gini impurity to decide on splitting.\n",
    "* The tree only supports numerical data (float, int etc.)\n",
    "* The tree only support categorical data if it is one hot encoded. This way a categorical feature can be used to split based on just two categories (0 or 1) which makes it a lot easier\n",
    "* The stopping criterion for splitting:\n",
    "    * If either a specified max depth has been reached or the number of samples at the node is less than or equal to a specified minimum required to split\n",
    "    * If all the labels(predictions) at the node are of the same class\n",
    "    * If the gini cost is not calculataed because a split is not possible (every split point results in a situation where one side of the split has no samples)\n",
    "* Logic\n",
    "    1. At the root create a node whose gini is calculated utilizing the whole training set. Assign the entire dataset (X and y values) to the root node. Set the root node as the current node\n",
    "    2. Loop through all features for the dataset portion for this node and try each one as a possible candidate for splitting\n",
    "    3. For each such feature if the feature is categorical (one hot encoded so values are 0 or 1) try splitting between all 0's and 1's and calculate cost\n",
    "    4. If the feature is numeric, then try each value in the column as a possible splitting point and calculate cost\n",
    "    5. From steps 3 and 4 pick the split with minimum cost. Create the left and the right children nodes for the current node based on this split\n",
    "    6. Then recursively repeat from step 2 for the left and right node\n",
    "    7. Stop when stopping criteria is reached\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a class that represents a node. The node class at minimum needs the following properties:\n",
    "1. The portions of the training dataset (X and y) associated with it. For the root node this will be the entire X and y.\n",
    "2. The parent node (None if node is root)\n",
    "3. The node depth\n",
    "4. The right child node\n",
    "5. The left child node\n",
    "6. The dominant class (the class in y that has the highest count). This will be used to predict the class of a test sample if this node becomes a leaf node and is is used for prediction\n",
    "7. The feature index (column index from X) on whihc this node is being split\n",
    "8. Property indicating whether the feature used to split is categorical or not\n",
    "9. The feature value at which the split occurs (only applicable for non-categorical columns. For categorical (one hot encoded) columns the split always occurs such that all 0's go left and the 1's go right)\n",
    "7. The gini score (this is not required - but good to keep around for each node for future visualization etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, parent = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # get the unique classes and their counts from the ground truth (y)\n",
    "        values,counts = np.unique(y,return_counts=True)\n",
    "        # set the class with the highest count as the class to be predicted using this node\n",
    "        self.predict_class = values[np.argmax(counts)]\n",
    "        # calculate gini for this node\n",
    "        self.gini = 1 - np.sum([(y[y == cls].shape[0]/y.shape[0])**2 for cls in values])       \n",
    "        self.parent = parent\n",
    "        # increment depth as needed\n",
    "        self.depth = 1 if parent is None else parent.depth + 1\n",
    "\n",
    "        self.right: Node = None\n",
    "        self.left: Node = None\n",
    "        self.feature_idx = None\n",
    "        self.categorical = None\n",
    "        self.split_at = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is the main tree class. \n",
    "1. The stopping parameters (if any) are passed in the constructor and stored in properties\n",
    "2. The tree also stores a pointer to the root node\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    # constructor\n",
    "    def __init__(self, max_depth = None, min_sample_split = None):\n",
    "        # initialize root node - we do not have one yet\n",
    "        self.head = None\n",
    "        # save the stopping parameters\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        # initialize the class list (we do not have this yet)\n",
    "        self.clsarr = None\n",
    "\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, categorical_col_idx: set[int]):\n",
    "        self.clsarr = np.unique(y)\n",
    "        self.head = Node(X,y)\n",
    "        self.__split(self.head,categorical_col_idx,None)\n",
    "\n",
    "    \n",
    "    def __split(self, cur_node: Node, categorical_col_idx: set[int], parent: Node = None):\n",
    "        \n",
    "        if (parent is not None and self.max_depth is not None and parent.depth + 1 > self.max_depth):\n",
    "            return\n",
    "        if(self.min_sample_split is not None and cur_node.X.shape[0] <= self.min_sample_split):\n",
    "            return\n",
    "        if (np.unique(cur_node.y).shape[0] == 1):\n",
    "            return\n",
    "\n",
    "        (cost,gini_left,gini_right,feature_idx,split_at) = self.__calculate_feature_to_split_based_on_min_cost(cur_node.X,cur_node.y,categorical_col_idx)\n",
    "        \n",
    "        if cost == None:\n",
    "            return\n",
    "        \n",
    "        if feature_idx in categorical_col_idx:\n",
    "            left_indices = np.argwhere(cur_node.X[:,feature_idx] == 0).flatten()\n",
    "            right_indices = np.argwhere(cur_node.X[:,feature_idx] == 1).flatten()\n",
    "        else:\n",
    "            left_indices = np.argwhere(cur_node.X[:,feature_idx] < split_at).flatten()\n",
    "            right_indices = np.argwhere(cur_node.X[:,feature_idx] >= split_at).flatten()\n",
    "\n",
    "        # if split is going to result in one side being \"no samples\" do not split\n",
    "        if (left_indices.shape[0] == 0 or right_indices.shape[0] == 0):\n",
    "            return\n",
    "        \n",
    "        cur_node.feature_idx = feature_idx\n",
    "        cur_node.categorical = feature_idx in categorical_col_idx\n",
    "        cur_node.split_at = split_at\n",
    "        \n",
    "        cur_node.left = Node(cur_node.X[left_indices,:],cur_node.y[left_indices],cur_node)\n",
    "        cur_node.right = Node(cur_node.X[right_indices,:],cur_node.y[right_indices],cur_node)\n",
    "\n",
    "        self.__split(cur_node.left,categorical_col_idx,cur_node)\n",
    "        self.__split(cur_node.right,categorical_col_idx,cur_node)\n",
    "\n",
    "\n",
    "    def __calculate_feature_to_split_based_on_min_cost(self, X: np.ndarray, y: np.ndarray, categorical_col_idx: set[int]):\n",
    "        cost_arr = np.empty((0,5))\n",
    "        for feature_idx in range(0,X.shape[1]):\n",
    "            xy = np.column_stack((X[:,feature_idx],y))\n",
    "        \n",
    "            if feature_idx in categorical_col_idx:\n",
    "                cost = self.__calculate_cost_and_gini(xy,feature_idx,categorical=True,split_point = None)\n",
    "                if cost[0,0] != None:\n",
    "                    cost_arr = np.concatenate((cost_arr,cost),axis=0)\n",
    "            else:                \n",
    "                possible_split_points = range(0,X.shape[0])\n",
    "                for split_point in possible_split_points:\n",
    "                    cost_for_split_point = self.__calculate_cost_and_gini(xy,feature_idx,categorical=False,split_point=split_point)\n",
    "                    if cost_for_split_point[0,0] != None:\n",
    "                        cost_arr = np.concatenate((cost_arr,cost_for_split_point))\n",
    "        \n",
    "        if cost_arr.shape[0] == 0:\n",
    "            return (None,None,None,None,None)\n",
    "        \n",
    "        min_cost_idx = np.argmin(cost_arr[:,0],axis = 0)\n",
    "        return cost_arr[min_cost_idx,0], cost_arr[min_cost_idx,1],cost_arr[min_cost_idx,2],int(cost_arr[min_cost_idx,3]),cost_arr[min_cost_idx,4]\n",
    "    \n",
    "    '''\n",
    "    Calculate the cost and gini impurity for a split\n",
    "\n",
    "    Arguments:\n",
    "        xy - n by 2 dataset where the first column is some feature(x) and the second column is the ground truth label (y)\n",
    "        feature_idx - column index of the feature(x) in the original training data\n",
    "        categorical - is the feature categorical ?\n",
    "        split_point - the row index pointing to the row where we want to split\n",
    "    Returns:\n",
    "        1x5 array of [cost,gini_left,gini_right,feature_idx,split_value (which is the x value pointed to by split_point and is None if the feature was categorical)]\n",
    "    '''\n",
    "    def __calculate_cost_and_gini(self,xy: np.ndarray, feature_idx: int, categorical: bool, split_point: int):\n",
    "        # to calculate gini and cost we only need the y values - so we split them\n",
    "        if categorical:\n",
    "            # left is all values of y where the x == 0, and right is all values of y where x == 1 \n",
    "            # (x is catagorical aka one hot encoded - so possible x values are 0 or 1)\n",
    "            y_left = xy[xy[:,0] == 0,1]\n",
    "            y_right = xy[xy[:,0] == 1,1]\n",
    "        else:\n",
    "            # left is all values of y where  x < (x value at row pointed to by split point) , \n",
    "            # and right is all values of y where  x >= (x value at row pointed to by split point) \n",
    "            y_left = xy[xy[:,0] < xy[split_point,0].item(),1]\n",
    "            y_right = xy[xy[:,0] >= xy[split_point,0].item(),1]\n",
    "\n",
    "        # a split was not possible at that split point because one or the other side resulted in no samples satisfying the split condition\n",
    "        # we return all(5) None values\n",
    "        if (y_left.shape[0] == 0 or y_right.shape[0] == 0):\n",
    "            return np.array([None]*5).reshape(1,-1)\n",
    "        # check the gini and cost formulae\n",
    "        # the probabilities for each class is just the number of y values of that class dividieed by the total number of samples\n",
    "        # to calculate gini we summ the squares of each class probability and subtract from 1\n",
    "        # The cost is then a weighted sum of the two gini values where the weights are the proportion of the number of instances \n",
    "        # in the left/right subset to the total number of samples \n",
    "        gini_left = 1 - np.sum([(y_left[y_left == cls].shape[0]/y_left.shape[0])**2 for cls in self.clsarr])\n",
    "        gini_right = 1 - np.sum([(y_right[y_right == cls].shape[0]/y_right.shape[0])**2 for cls in self.clsarr])\n",
    "        # return as a 1x5 array of [cost,gini_left,gini_right,feature_idx,split_value (which is the x value pointed to by split_point and is None if the feature was categorical)]\n",
    "        return np.array([(gini_left * y_left.shape[0]/xy.shape[0]) + (gini_right * y_right.shape[0]/xy.shape[0]),gini_left,gini_right,feature_idx,xy[split_point,0].item() if not categorical else None]).reshape(1,-1)\n",
    "    \n",
    "    def predict(self,X:np.ndarray) -> np.ndarray:\n",
    "        return np.apply_along_axis(self.__predict_internal,axis=1,arr=X,use_node = self.head)\n",
    "\n",
    "    def __predict_internal(self, X:np.ndarray, use_node: Node) -> float:\n",
    "        if (use_node.left is None and use_node.right is None):\n",
    "            return use_node.predict_class\n",
    "        elif use_node.categorical:\n",
    "            return self.__predict_internal(X,use_node.left if X[use_node.feature_idx].item() == 0 else use_node.right)\n",
    "        else:\n",
    "            return self.__predict_internal(X,use_node.left if X[use_node.feature_idx].item() < use_node.split_at else use_node.right)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"c:/pix/ml/titanic/train.csv\")\n",
    "df_data = df_data.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"],axis=1).dropna()\n",
    "\n",
    "df_X = df_data.drop(\"Survived\",axis=1)\n",
    "df_y = df_data[\"Survived\"]\n",
    "df_X = pd.get_dummies(df_X,columns=[\"Pclass\",\"Sex\",\"Embarked\"])\n",
    "df_X = df_X.astype(float)\n",
    "\n",
    "train_X,test_X,train_y,test_y = train_test_split(df_X,df_y,test_size=0.15,stratify=df_y,shuffle=True,random_state=1234)\n",
    "\n",
    "tree = DecisionTreeClassifier(min_sample_split=12, max_depth=16)\n",
    "tree.fit(train_X.values,train_y.values,categorical_col_idx=range(4,12))\n",
    "predictions_train = tree.predict(train_X.values)\n",
    "print(f\"train accuracy = {accuracy_score(predictions_train,train_y.values)}\")\n",
    "print(f\"train precision = {precision_score(predictions_train,train_y.values)}\")\n",
    "print(f\"train recall = {recall_score(predictions_train,train_y.values)}\")\n",
    "predictions_test = tree.predict(test_X.values)\n",
    "print(f\"test accuracy = {accuracy_score(predictions_test,test_y.values)}\")\n",
    "print(f\"test precision = {precision_score(predictions_test,test_y.values)}\")\n",
    "print(f\"test recall = {recall_score(predictions_test,test_y.values)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
